{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import tempfile\n",
    "import whisper\n",
    "from pytubefix import YouTube\n",
    "import openai\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import re  # Add this import to sanitize the score\n",
    "\n",
    "# Load API keys and environment variables\n",
    "load_dotenv('KEY_FILE.env')\n",
    "client = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# YouTube video URL\n",
    "YOUTUBE_VIDEO = \"https://www.youtube.com/watch?v=WX7DBPcsiEs&t=16s\"\n",
    "\n",
    "# Whisper transcription setup\n",
    "try:\n",
    "    whisper_model = whisper.load_model(\"base\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Whisper model: {e}\")\n",
    "    whisper_model = None\n",
    "\n",
    "# Step 1: Download YouTube audio and transcribe\n",
    "def download_and_transcribe(video_url):\n",
    "    \"\"\"\n",
    "    Downloads the audio from a YouTube video and generates its transcript.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        youtube = YouTube(video_url)\n",
    "        audio = youtube.streams.filter(only_audio=True).first()\n",
    "        if not audio:\n",
    "            raise ValueError(\"No audio stream available.\")\n",
    "        \n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            audio_file = audio.download(output_path=tmpdir)\n",
    "            transcription = whisper_model.transcribe(audio_file, fp16=False, task=\"translate\")[\"text\"].strip()\n",
    "            \n",
    "            # Save transcription to a file\n",
    "            with open(\"youtube_transcription.txt\", \"w\") as file:\n",
    "                file.write(transcription)\n",
    "            \n",
    "        return transcription\n",
    "    except Exception as e:\n",
    "        print(f\"Error during transcription: {e}\")\n",
    "        return None\n",
    "\n",
    "# Step 2: Chunk transcript into manageable pieces\n",
    "def split_transcript(text, max_length=1500, overlap=200):\n",
    "    \"\"\"\n",
    "    Splits the transcript into chunks with optional overlap for context retention.\n",
    "    \"\"\"\n",
    "    sentences = text.split('. ')\n",
    "    chunks, current_chunk = [], \"\"\n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) < max_length:\n",
    "            current_chunk += sentence + \". \"\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence + \". \"\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    return chunks\n",
    "\n",
    "# Step 3: Analyze chunks with OpenAI\n",
    "\n",
    "\n",
    "def detect_toxicity_openai(chunks):\n",
    "    \"\"\"\n",
    "    Analyzes chunks of text for harmful or toxic content using OpenAI.\n",
    "    \"\"\"\n",
    "    toxicity_results = []\n",
    "    for chunk in chunks:\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a content moderation assistant. Analyze the following text for harmful or toxic content. Provide a toxicity score between 0 (non-toxic) and 1 (highly toxic), and explain your reasoning.\"},\n",
    "                    {\"role\": \"user\", \"content\": chunk}\n",
    "                ]\n",
    "            )\n",
    "            result = response.choices[0].message.content\n",
    "            \n",
    "            # Extract toxicity score safely\n",
    "            if \"Toxicity Score:\" in result:\n",
    "                score_raw = result.split(\"Toxicity Score:\")[1].split()[0]\n",
    "                # Remove non-numeric characters like \"**\" using regex\n",
    "                sanitized_score = re.sub(r\"[^\\d.]\", \"\", score_raw)\n",
    "                toxicity_score = float(sanitized_score)\n",
    "            else:\n",
    "                toxicity_score = 0.0  # Default to non-toxic if score is missing\n",
    "            \n",
    "            toxicity_results.append({\"chunk\": chunk, \"toxicity_score\": toxicity_score, \"result\": result})\n",
    "        except openai.OpenAIError as e:\n",
    "            print(f\"Error with OpenAI API: {e}\")\n",
    "            toxicity_results.append({\"chunk\": chunk, \"toxicity_score\": -1, \"result\": \"Error processing chunk\"})\n",
    "        except ValueError as e:\n",
    "            print(f\"Error parsing toxicity score: {e}\")\n",
    "            toxicity_results.append({\"chunk\": chunk, \"toxicity_score\": -1, \"result\": \"Invalid score format\"})\n",
    "        time.sleep(2)  # Delay to avoid hitting API rate limits\n",
    "    return toxicity_results\n",
    "\n",
    "\n",
    "\n",
    "# Step 4: Visualize sentiment heatmap\n",
    "def visualize_heatmap(toxicity_results):\n",
    "    \"\"\"\n",
    "    Generates a heatmap of toxicity scores for the transcript chunks.\n",
    "    \"\"\"\n",
    "    scores = [result[\"toxicity_score\"] for result in toxicity_results if result[\"toxicity_score\"] >= 0]\n",
    "    plt.figure(figsize=(10, 2))\n",
    "    sns.heatmap([scores], cmap=\"Reds\", cbar=True, xticklabels=range(len(scores)), yticklabels=[\"Toxicity\"])\n",
    "    plt.title(\"Sentiment Heatmap of Transcript\")\n",
    "    plt.savefig(\"toxicity_heatmap.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Step 5: Analyze results and flag harmful content\n",
    "def analyze_and_flag(toxicity_results):\n",
    "    \"\"\"\n",
    "    Flags chunks with high toxicity and calculates a risk score.\n",
    "    \"\"\"\n",
    "    flagged_chunks = [result for result in toxicity_results if result[\"toxicity_score\"] > 0.5]\n",
    "    risk_score = (len(flagged_chunks) / len(toxicity_results)) * 100\n",
    "    return flagged_chunks, risk_score\n",
    "\n",
    "# Step 6: Save flagged content and generate report\n",
    "def generate_report(flagged_chunks, risk_score):\n",
    "    \"\"\"\n",
    "    Generates a detailed report of flagged content and saves it to a file.\n",
    "    Includes a summary even if no harmful content is flagged.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(\"flagged_transcript.txt\", \"w\") as file:\n",
    "            if flagged_chunks:\n",
    "                file.write(\"Flagged Content Analysis Report\\n\")\n",
    "                file.write(f\"Risk Score: {risk_score:.2f}% harmful content flagged.\\n\\n\")\n",
    "                for chunk in flagged_chunks:\n",
    "                    file.write(f\"Toxicity Score: {chunk['toxicity_score']}\\n\")\n",
    "                    file.write(f\"Chunk: {chunk['chunk']}\\n\")\n",
    "                    file.write(f\"Result: {chunk['result']}\\n\\n\")\n",
    "            else:\n",
    "                file.write(\"Flagged Content Analysis Report\\n\")\n",
    "                file.write(\"No harmful content was detected in the video.\\n\")\n",
    "                file.write(f\"Risk Score: {risk_score:.2f}% harmful content flagged.\\n\\n\")\n",
    "                file.write(\"The content of the video appears to be safe and within acceptable limits.\\n\")\n",
    "        print(\"Report saved to 'flagged_transcript.txt'.\")\n",
    "        print(f\"Risk Score: {risk_score:.2f}% harmful content flagged.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating report: {e}\")\n",
    "\n",
    "\n",
    "# Main workflow\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to execute the video transcription and toxicity analysis workflow.\n",
    "    \"\"\"\n",
    "    if not whisper_model:\n",
    "        print(\"Whisper model could not be loaded. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Step 1: Transcribe video\n",
    "    print(\"Transcribing YouTube video...\")\n",
    "    transcript = download_and_transcribe(YOUTUBE_VIDEO)\n",
    "    if not transcript:\n",
    "        print(\"Failed to transcribe video. Exiting.\")\n",
    "        return\n",
    "    print(\"Transcription completed.\")\n",
    "    \n",
    "    # Step 2: Chunk transcript\n",
    "    print(\"Chunking transcript...\")\n",
    "    chunks = split_transcript(transcript)\n",
    "    \n",
    "    # Step 3: Detect toxicity\n",
    "    print(\"Detecting toxicity with OpenAI...\")\n",
    "    toxicity_results = detect_toxicity_openai(chunks)\n",
    "    \n",
    "    # Step 4: Visualize heatmap of toxicity scores\n",
    "    print(\"Visualizing heatmap...\")\n",
    "    visualize_heatmap(toxicity_results)\n",
    "    \n",
    "    # Step 5: Analyze and flag content\n",
    "    print(\"Analyzing results...\")\n",
    "    flagged_chunks, risk_score = analyze_and_flag(toxicity_results)\n",
    "    \n",
    "    # Step 6: Generate report\n",
    "    generate_report(flagged_chunks, risk_score)\n",
    "    \n",
    "    # Print warnings if risk score exceeds threshold\n",
    "    threshold = 30\n",
    "    if risk_score > threshold:\n",
    "        print(f\"Warning: The video contains {risk_score:.2f}% harmful or violent content.\")\n",
    "    else:\n",
    "        print(f\"The video contains {risk_score:.2f}% harmful content, which is within acceptable limits.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
